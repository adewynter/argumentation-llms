{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601c63dd",
   "metadata": {},
   "source": [
    "# Available prompts\n",
    "All prompts take in two arguments: point to be evaluated, and exemplars. Exemplars may be `None`.\n",
    "\n",
    "- AM prompts take in three arguments: point, exemplar set, and a boolean (`is_reply`) to load the review or the reply.\n",
    "- AMR prompts take the AM arguments with some changes:\n",
    "  - The point must be a tuple of the point _and_ the AMR graph.\n",
    "  - The fourth argument is a boolean (`include_context`). If set to `True`, it returns the sentence and the graph; the graph only otherwise.\n",
    "\n",
    "| Prompt name | Problem | Domain | What it does | \n",
    "|-------------|---------------|---------------|---------------|\n",
    "| build_prompt_symbolic | AM | Symbolic | Symbolic with BIO tags as return signature |\n",
    "| build_prompt_symbolic_w_numbers | AM | Symbolic | Symbolic with indices as return signature |\n",
    "| build_prompt_symbolic_w_numbers_in_lines | AM | Symbolic | Symbolic with indices as return signature _and_ indices marked inline |\n",
    "| build_prompt_with_amr | AM  | Symbolic (AMR) | All AMR prompts go here, with context and without context |\n",
    "| build_prompt_symbolic_w_numbers_in_lines_cot | AM | Symbolic | CoT symbolic with inline numbers and inline return signature |\n",
    "| build_prompt_symbolic_w_numbers_cot | AM | Symbolic | CoT symbolic with inline numbers |\n",
    "| build_prompt_symbolic_cot | AM | Symbolic | CoT with BIO tags as return signature |\n",
    "|-------------|---------------|---------------|---------------|\n",
    "| build_prompt_pair_symbolic_ape | APE | Symbolic | Symbolic with indices as return signature | \n",
    "| build_prompt_pair_symbolic_w_numbers | APE | Symbolic | Symbolic with indices as return signature _and_ indices marked inline |\n",
    "| build_prompt_pair_symbolic_full | APE | Symbolic | Symbolic with the binary matrix as return signature |\n",
    "|-------------|---------------|---------------|---------------|\n",
    "| build_prompt_pair | AM | Concrete | Concrete prompt, both reviews and passages in a zero shot manner (unused) | \n",
    "| build_prompt_single | AM | Concrete | Concrete prompt, one per review/rebuttal passage | \n",
    "| build_prompt_single_cot | AM | Concrete | CoT |\n",
    "| build_prompt_pair_ape | APE | Concrete | APE concrete |\n",
    "|-------------|---------------|---------------|---------------|\n",
    "| build_prompt_pair_x_symbolic | AM and APE | Symbolic | Use AM first and then perform APE concrete. Not in the paper due to low performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf24a92-2371-4f98-9c58-8b1748909625",
   "metadata": {},
   "source": [
    "## PLEASE NOTE:\n",
    "- You need to bring your own LLMClient class, such as [OpenAI API](https://openai.com/product#made-for-developers)\n",
    "  - You should be able to update the `max_tokens` parameter on the fly, and support a `send_request` method that sends text and returns a JSON object like OpenAI's.\n",
    "- For amrlib: `pip install amrlib` and install [the model](https://amrlib.readthedocs.io/en/latest/install/).\n",
    "  - We used `model_parse_xfm_bart_large-v0_1_0` since its SMATCH is around the same as the SOTA (83 vs 85) and works with this library.\n",
    "- Please note you will need the data in JSON format from [here](https://github.com/LiyingCheng95/MLMC/tree/main/data/rr-submission-v2) (only `test.json`, `dev.json`)\n",
    "- We have only provided a random sample of the model responses in all categories since the full annotations + dataset are 3 GB.\n",
    "  - Full outputs and data will be released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f75688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llmclient import LLMClient\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tiktoken # Needed for max-length tokenisation\n",
    "\n",
    "from loading_code_rrv2 import *\n",
    "from prompt_utils import *\n",
    "\n",
    "from prompts_amr import *\n",
    "from prompts_symbolic import *\n",
    "from prompts_concrete import *\n",
    "\n",
    "enc_gpt4 = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "enc_gpt3 = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "\n",
    "processed = load_data_instances(json.load(open(\"test.json\", \"r\", encoding=\"utf-8\")), -1, False)\n",
    "dev = load_data_instances(json.load(open(\"dev.json\", \"r\", encoding=\"utf-8\")), -1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f144b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\"max_tokens\": 756,\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 1,\n",
    "                \"n\": 5,\n",
    "                \"frequency_penalty\":0,\n",
    "                \"presence_penalty\":0,\n",
    "                \"logprobs\": None,\n",
    "                \"stop\": None}\n",
    "\n",
    "# Each task takes in slightly different max_tokens.\n",
    "# We tune these prior to every call and during experimentation.\n",
    "# For example, CoT needs around 1500 tokens.\n",
    "# Concrete approaches ~1000.\n",
    "# Symbolic between 256 and 758.\n",
    "# Everything else stays the same across all experiments.\n",
    "llm_client_gpt4 = LLMClient(request_data, \"GPT-4\")\n",
    "llm_client_gpt3 = LLMClient(request_data, \"GPT-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd3f30",
   "metadata": {},
   "source": [
    "# Example call: GPT-4 Concrete (AM)\n",
    "- Sample data-gathering call for AM (only the rebuttal)\n",
    "- zero-shot-token call\n",
    "- Code will perpetually retry until done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "output_filename = \"CoT/gpt4_reply_cot_symbolic_w_numbers_in_lines_zero_shot.json\"\n",
    "ffailed = [i for i in range(len(processed))]\n",
    "rounds = 0\n",
    "\n",
    "while ffailed != []:\n",
    "    failed = [] \n",
    "    for i in tqdm(range(len(processed))):\n",
    "        if i not in ffailed:\n",
    "            continue\n",
    "        instance = processed[i]\n",
    "        examples = None # exemplars go here (e.g., dev[:4] to do 4-shot)\n",
    "        prompt = build_prompt_symbolic_w_numbers_in_lines_cot(instance, examples, is_reply=True)\n",
    "        processed_response = {\"prompt\": prompt,\n",
    "                              \"index\": i,\n",
    "                              \"model_responses\": {},\n",
    "                              \"actuals_text\": [s.replace(\"<sep>\", \"\").strip() for l,s in zip(instance.review_bio, instance.review) if l == 1 or l == 2],\n",
    "                              \"actuals_bio\": [l.item() for l in instance.review_bio]\n",
    "                              }\n",
    "        try:\n",
    "            response = llm_client_gpt4.send_request(prompt)\n",
    "        except:\n",
    "            failed.append((i, response))\n",
    "        if \"choices\" in response:\n",
    "            for i, choice in enumerate(response[\"choices\"]):\n",
    "                processed_response[\"model_responses\"][\"try\" + str(i)] = choice[\"text\"]\n",
    "        else:\n",
    "            failed.append((i, response))\n",
    "        with open(output_filename, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(processed_response) + \"\\n\")\n",
    "\n",
    "    if failed != []:\n",
    "        print([v[0] for v in failed])\n",
    "        print(failed[-1])\n",
    "        ffailed = [v[0] for v in failed]\n",
    "        rounds += 1\n",
    "    else:\n",
    "        print(failed)\n",
    "        print(len(failed))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849fcb9",
   "metadata": {},
   "source": [
    "# Large Parallel Requests\n",
    "Sometimes the instance will not be able to allocate enough RAM for requests calling a best-of-5 using up all 32k tokens. This is the workaround.\n",
    "- It is very slow, but it works.\n",
    "- Sample symbolic with numbers and CoT at 16k (max) examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d51ce8-74b3-4c61-a275-2613bc48256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data = {\"max_tokens\": 756,\n",
    "                \"temperature\": 0.8,\n",
    "                \"top_p\": 1,\n",
    "                \"n\": 1,\n",
    "                \"frequency_penalty\":0,\n",
    "                \"presence_penalty\":0,\n",
    "                \"logprobs\": None,\n",
    "                \"stop\": None}\n",
    "\n",
    "# Same as before, each task takes in slightly different max_tokens.\n",
    "# Everything else stays the same across all experiments.\n",
    "# These parameters return a single response per call, so it'll take five times longer.\n",
    "llm_client_gpt4 = LLMClient(request_data, \"GPT-4\")\n",
    "llm_client_gpt3 = LLMClient(request_data, \"GPT-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd25fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "ffailed = [i for i in range(len(processed))]\n",
    "rounds = 0\n",
    "is_reply = False\n",
    "\n",
    "suff = \"reply\" if is_reply else \"review\"\n",
    "output_filename = \"CoT/gpt4_{}_symbolic_w_numbers_in_lines_16k.json\".format(suff)\n",
    "\n",
    "while ffailed != []:\n",
    "    failed = []\n",
    "    too_long_tokens = []\n",
    "    for i in tqdm(range(len(processed))):\n",
    "        if i not in ffailed:\n",
    "            continue\n",
    "        instance = processed[i]\n",
    "        examples = dev[0:16]\n",
    "        prompt = build_prompt_symbolic_w_numbers_in_lines_cot(instance, examples, is_reply=is_reply)\n",
    "\n",
    "        max_examples = 16\n",
    "        if len(enc_gpt4.encode(prompt)) + request_data[\"max_tokens\"] > 32_000:\n",
    "            while True:\n",
    "                examples = dev[0:max_examples]\n",
    "                prompt = build_prompt_symbolic_w_numbers_in_lines_cot(instance, examples, is_reply=is_reply)\n",
    "                if len(enc_gpt4.encode(prompt)) + request_data[\"max_tokens\"] > 32_000:\n",
    "                    max_examples = max_examples - 1\n",
    "                else:\n",
    "                    break\n",
    "        examples = dev[0:max_examples]\n",
    "        prompt = build_prompt_symbolic_w_numbers_in_lines_cot(instance, examples, is_reply=is_reply)\n",
    "        processed_response = {\"prompt\": prompt,\n",
    "                              \"index\": i,\n",
    "                              \"model_responses\": {},\n",
    "                              \"actuals_text\": [s.replace(\"<sep>\", \"\").strip() for l,s in zip(instance.review_bio, instance.review) if l == 1 or l == 2],\n",
    "                              \"actuals_bio\": [l.item() for l in instance.review_bio]\n",
    "                              }\n",
    "        \n",
    "        not_finished = False\n",
    "        for k in range(5):\n",
    "            try:\n",
    "                response = llm_client_gpt4.send_request(prompt)\n",
    "            except:\n",
    "                failed.append((i, response))\n",
    "            if \"choices\" in response:\n",
    "                processed_response[\"model_responses\"][\"try\" + str(k)] = response[\"choices\"][0][\"text\"]\n",
    "            else:\n",
    "                failed.append((i, response))\n",
    "                not_finished = True\n",
    "                if 'error' in response:\n",
    "                    if response[\"error\"][\"code\"] != \"InternalServerError\":\n",
    "                        too_long_tokens.append((i, response))\n",
    "                break\n",
    "\n",
    "        if not not_finished:\n",
    "            with open(output_filename, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(processed_response) + \"\\n\")\n",
    "\n",
    "    if too_long_tokens != []:\n",
    "        print(\"Too long:\")\n",
    "        print([v[0] for v in too_long_tokens])\n",
    "        print(\"\")\n",
    "    if failed != []:\n",
    "        print([v[0] for v in failed])\n",
    "        print(failed[-1])\n",
    "        ffailed = [v[0] for v in failed]\n",
    "        rounds += 1\n",
    "    else:\n",
    "        print(failed)\n",
    "        print(len(failed))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce67a47",
   "metadata": {},
   "source": [
    "# AMR Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64813fdd",
   "metadata": {},
   "source": [
    "We need to first generate the data and then make the call, otherwise it will be incredibly slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0cdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import amrlib\n",
    "\n",
    "# Please download the model ahead of time. We use https://github.com/bjascob/amrlib/ for this.\n",
    "parser_model = \"model_parse_xfm_bart_large-v0_1_0\"\n",
    "stog = amrlib.load_stog_model(model_dir=parser_model)\n",
    "\n",
    "# Dump to AMR\n",
    "for i in tqdm(range(len(processed))):\n",
    "    point = processed[i]\n",
    "    pt = {\"index\": i,\n",
    "          \"review\": point.review,\n",
    "          \"response\": point.reply}\n",
    "    amr_review_graphs = stog.parse_sents(point.review, add_metadata=True)\n",
    "    amr_response_graphs = stog.parse_sents(point.reply, add_metadata=True)\n",
    "    pt[\"review_amr_graphs\"] = amr_review_graphs\n",
    "    pt[\"amr_response_graphs\"] = amr_response_graphs\n",
    "    with open(\"AMR_graphs/rrv2_amr_line_by_line.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(pt) + \"\\n\")\n",
    "        \n",
    "# Build your exemplar set.\n",
    "# Dump to AMR -- you really don't need 100 AMR graphs. Like 20 would do.\n",
    "for i in tqdm(range(100)):\n",
    "    point = dev[i]\n",
    "    pt = {\"index\": i,\n",
    "          \"review\": point.review,\n",
    "          \"response\": point.reply}\n",
    "    amr_review_graphs = stog.parse_sents(point.review, add_metadata=True)\n",
    "    amr_response_graphs = stog.parse_sents(point.reply, add_metadata=True)\n",
    "    pt[\"review_amr_graphs\"] = amr_review_graphs\n",
    "    pt[\"amr_response_graphs\"] = amr_response_graphs\n",
    "    with open(\"AMR_graphs/rrv2_amr_dev_line_by_line.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(pt) + \"\\n\")\n",
    "        \n",
    "txt = processed[0].review\n",
    "graphs = stog.parse_sents(txt, add_metadata=True)\n",
    "for graph in graphs:\n",
    "    print(json.dumps(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d488956",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = load_data_instances(json.load(open(\"test.json\", \"r\", encoding=\"utf-8\")), -1, False)\n",
    "dev = load_data_instances(json.load(open(\"dev.json\", \"r\", encoding=\"utf-8\")), -1, False)\n",
    "all_graphs_test = [json.loads(l) for l in open(\"AMR_graphs/rrv2_amr_line_by_line.json\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "all_graphs_dev = [json.loads(l) for l in open(\"AMR_graphs/rrv2_amr_dev_line_by_line.json\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "responses = []\n",
    "failed = []\n",
    "is_reply = False\n",
    "include_context = False\n",
    "\n",
    "suff = \"reply\" if is_reply else \"review\"\n",
    "output_filename = \"AM/gpt4_{}_amr_no_context_32k.json\".format(suff)\n",
    "\n",
    "graphs = [k[\"amr_response_graphs\"] for k in all_graphs_dev] if is_reply else [k[\"review_amr_graphs\"] for k in all_graphs_dev]\n",
    "all_examples = [(d, g) for d, g in zip(dev, graphs)]\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(processed))):\n",
    "    instance = [processed[i], all_graphs_test[i][\"amr_response_graphs\"]] if is_reply else [processed[i], all_graphs_test[i][\"review_amr_graphs\"]]\n",
    "    examples = all_examples[:10]\n",
    "\n",
    "    prompt = build_prompt_with_amr(instance, examples, is_reply=is_reply, include_context=include_context)\n",
    "    processed_response = {\"prompt\": prompt,\n",
    "                          \"index\": i,\n",
    "                          \"model_responses\": {},\n",
    "                          \"amrs\": instance[-1],\n",
    "                          \"full_context\": include_context\n",
    "                          }\n",
    "    try:\n",
    "        response = llm_client_gpt4.send_request(prompt)\n",
    "    except:\n",
    "        failed.append((i, response))\n",
    "    if \"choices\" in response:\n",
    "        for i, choice in enumerate(response[\"choices\"]):\n",
    "            processed_response[\"model_responses\"][\"try\" + str(i)] = choice[\"text\"]\n",
    "    else:\n",
    "        failed.append((i, response))\n",
    "    with open(output_filename, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(processed_response) + \"\\n\")\n",
    "\n",
    "if failed != []:\n",
    "    print([v[0] for v in failed])\n",
    "    print(failed[-1])\n",
    "else:\n",
    "    print(failed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83695a",
   "metadata": {},
   "source": [
    "# Extra: Cross-AM/APE Approaches\n",
    "- Not in the paper due to low performance. Example below uses zero-shot AM responses to build (a zero-shot) APE response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = load_data_instances(json.load(open(\"test.json\", \"r\", encoding=\"utf-8\")), -1, False)\n",
    "dev = load_data_instances(json.load(open(\"dev.json\", \"r\", encoding=\"utf-8\")), -1, False)\n",
    "mined = load_to_example(processed, collator(resps[\"0k\"][\"review\"], returnit=True), collator(resps[\"0k\"][\"response\"], returnit=True))\n",
    "\n",
    "responses = []\n",
    "failed = []\n",
    "\n",
    "for i in tqdm(range(len(processed))):\n",
    "    instance = mined[i]\n",
    "    examples = None\n",
    "    prompt = build_prompt_pair_x_symbolic(instance, examples)\n",
    "    processed_response = {\"prompt\": prompt,\n",
    "                          \"index\": i,\n",
    "                          \"model_responses\": {},\n",
    "                          \"instance\": instance,\n",
    "                          }\n",
    "    try:\n",
    "        response = llm_client_gpt4.send_request(prompt)\n",
    "    except:\n",
    "        failed.append((i, response))\n",
    "    if \"choices\" in response:\n",
    "        for i, choice in enumerate(response[\"choices\"]):\n",
    "            processed_response[\"model_responses\"][\"try\" + str(i)] = choice[\"text\"]\n",
    "    else:\n",
    "        failed.append((i, response))\n",
    "    with open(\"gpt4_ape_x_symbolic_zero_shot_zero_shot.json\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(processed_response) + \"\\n\")\n",
    "print(failed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
